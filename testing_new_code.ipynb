{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQG3nv-akaxy",
        "outputId": "42f7690b-045c-4c6d-bb32-d2c94b5e75f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Collecting medmnist\n",
            "  Downloading medmnist-3.0.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.2.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.24.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from medmnist) (4.66.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from medmnist) (11.0.0)\n",
            "Collecting fire (from medmnist)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.20.1+cu121)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2024.2)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (3.4.2)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2.36.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2024.9.20)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.16.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->medmnist) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Downloading medmnist-3.0.2-py3-none-any.whl (25 kB)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=326d7a13c0f3227a82c9c0e3b2b735bf3e2e344b723be1977e719d6676118647\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
            "Successfully built fire\n",
            "Installing collected packages: findspark, fire, medmnist\n",
            "Successfully installed findspark-2.0.1 fire-0.7.0 medmnist-3.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark medmnist tensorflow findspark scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from medmnist import INFO, ChestMNIST\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ChestMNIST with PySpark\") \\\n",
        "    .getOrCreate()\n",
        "print(\"spark created\")\n",
        "\n",
        "# Choose chest x-ray data from medmnist library\n",
        "data_flag = 'chestmnist'\n",
        "info = INFO[data_flag]\n",
        "print(\"data chosed\")\n",
        "\n",
        "# Load the dataset using MedMNIST library\n",
        "train_dataset = ChestMNIST(split='train', download=True)\n",
        "val_dataset = ChestMNIST(split='val', download=True)\n",
        "test_dataset = ChestMNIST(split='test', download=True)\n",
        "print(\"dataset loaded\")\n",
        "\n",
        "# Extract the pixel data\n",
        "X_train, X_val, X_test = train_dataset.imgs, val_dataset.imgs, test_dataset.imgs\n",
        "y_train, y_val, y_test = train_dataset.labels, val_dataset.labels, test_dataset.labels\n",
        "print(\"data extracted\")\n",
        "\n",
        "# ** Data Exploration **\n",
        "\n",
        "# Normalize pixel values\n",
        "X_train, X_val, X_test = X_train / 255.0, X_val / 255.0, X_test / 255.0\n",
        "print(\"data normalized\")\n",
        "\n",
        "# Add another dimension for CNN model to be able to process the data.\n",
        "X_train = X_train[..., np.newaxis]\n",
        "X_val = X_val[..., np.newaxis]\n",
        "X_test = X_test[..., np.newaxis]\n",
        "print(\"added dimension\")\n",
        "# Convert to Spark DataFrame\n",
        "def convert_to_spark_df(X, y):\n",
        "    # Flatten the image data to a single vector\n",
        "    X_flattened = X.reshape(X.shape[0], -1)\n",
        "    print(\"image flattened\")\n",
        "    # Ensure y is reshaped correctly to be a single column of labels for each sample\n",
        "    # If y is multi-label, make sure it's the correct shape (num_samples, num_labels)\n",
        "    if y.ndim > 1:\n",
        "        y = y.argmax(axis=1).reshape(-1, 1)  # If multi-label, use the argmax for multi-class classification\n",
        "\n",
        "    # Create a Spark DataFrame with features and labels\n",
        "    data = np.hstack((X_flattened, y))  # Combine flattened features and labels\n",
        "    columns = [f\"feature_{i}\" for i in range(X_flattened.shape[1])] + [\"label\"]\n",
        "    print(\"spark dataframe created\")\n",
        "    return spark.createDataFrame(data.tolist(), columns)\n",
        "\n",
        "train_data = convert_to_spark_df(X_train, y_train)\n",
        "val_data = convert_to_spark_df(X_val, y_val)\n",
        "test_data = convert_to_spark_df(X_test, y_test)\n",
        "\n",
        "# ** Feature Engineering with PySpark **\n",
        "\n",
        "# VectorAssembler to combine all features into a single vector column for scaling\n",
        "feature_cols = [f\"feature_{i}\" for i in range(X_train.shape[1])]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "print(\"combined features into vector\")\n",
        "\n",
        "# StandardScaler for feature scaling\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
        "print(\"applied standard scaler\")\n",
        "\n",
        "# ** Pipeline for Data Preprocessing **\n",
        "\n",
        "# Create pipeline for transformations\n",
        "pipeline = Pipeline(stages=[assembler, scaler])\n",
        "print(\"pipeline created\")\n",
        "\n",
        "# Fit and transform the training data using PySpark’s StandardScaler\n",
        "scaler_model = pipeline.fit(train_data)\n",
        "train_data = scaler_model.transform(train_data)\n",
        "val_data = scaler_model.transform(val_data)\n",
        "test_data = scaler_model.transform(test_data)\n",
        "print(\"fit and transformed training data\")\n",
        "# ** Data Rebalancing with PySpark **\n",
        "\n",
        "def rebalance_data_spark(X, y):\n",
        "    # Convert to Spark DataFrame\n",
        "    data = np.hstack((X, y.reshape(-1, 1)))\n",
        "    columns = [f\"feature_{i}\" for i in range(X.shape[1])] + [\"label\"]\n",
        "    df = spark.createDataFrame(data.tolist(), columns)\n",
        "    print(\"spark dataframe created after rebalancing\")\n",
        "\n",
        "    # Split data into positive and negative classes\n",
        "    negative_df = df.filter(df[\"label\"] == 0)\n",
        "    positive_df = df.filter(df[\"label\"] == 1)\n",
        "    print(\"data split after rebalancing\")\n",
        "\n",
        "    # Over-sample the positive class\n",
        "    positive_oversampled = positive_df.sample(True, fraction=negative_df.count() / positive_df.count(), seed=42)\n",
        "    print(\"oversampled data\")\n",
        "\n",
        "    # Combine and shuffle the dataset\n",
        "    balanced_df = negative_df.union(positive_oversampled)\n",
        "    print(\"combined and shuffled rebalanced dataset\")\n",
        "    return balanced_df.sample(False, 1.0, seed=42)  # shuffle\n",
        "\n",
        "balanced_train_data = rebalance_data_spark(X_train, y_train)\n",
        "\n",
        "# ** Convert Data to Numpy for TensorFlow Training **\n",
        "\n",
        "def spark_to_numpy(df):\n",
        "    features = np.array(df.select(\"scaled_features\").rdd.map(lambda row: row[0].toArray()).collect())\n",
        "    labels = np.array(df.select(\"label\").rdd.map(lambda row: row[0]).collect())\n",
        "    print(\"converted features and labels into numpy arrays\")\n",
        "    return features, labels\n",
        "\n",
        "# Convert the rebalanced data back to Numpy arrays for TensorFlow\n",
        "X_train_balanced, y_train_balanced = spark_to_numpy(balanced_train_data)\n",
        "X_val, y_val = spark_to_numpy(val_data)\n",
        "X_test, y_test = spark_to_numpy(test_data)\n",
        "\n",
        "# ** CNN Model Definition with TensorFlow **\n",
        "\n",
        "# Define the CNN model using keras\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "print(\"cnn model defined\")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "print(\"cnn model compiled\")\n",
        "\n",
        "# Train the Model using Keras\n",
        "history = model.fit(X_train_balanced, y_train_balanced, epochs=100, batch_size=32, validation_data=(X_val, y_val))\n",
        "print(\"cnn model trained\")\n",
        "\n",
        "# ** Model Evaluation **\n",
        "\n",
        "# Evaluate the Model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_acc:.2f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_binary, display_labels=['Negative', 'Positive'])\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_binary, target_names=['Negative', 'Positive']))\n",
        "\n",
        "# Plot accuracy and loss over the epochs to view model performance\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDyi-gEikehb",
        "outputId": "7672597b-3bef-481f-c857-8ca3a9f3cf81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark created\n",
            "data chosed\n",
            "Downloading https://zenodo.org/records/10519652/files/chestmnist.npz?download=1 to /root/.medmnist/chestmnist.npz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 82.8M/82.8M [00:07<00:00, 11.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/chestmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/chestmnist.npz\n",
            "dataset loaded\n",
            "data extracted\n",
            "data normalized\n",
            "added dimension\n",
            "image flattened\n",
            "spark dataframe created\n",
            "image flattened\n",
            "spark dataframe created\n",
            "image flattened\n",
            "spark dataframe created\n",
            "combined features into vector\n",
            "applied standard scaler\n",
            "pipeline created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hLoWhq8rBud1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}